"""
Gestionnaire de mod√®le Qwen3-VL avec vLLM pour performance optimale
"""
import asyncio
import os
import time
import logging
from typing import Dict, Any, List, Optional, AsyncGenerator, Union
import json

try:
    from vllm import LLM, SamplingParams
    from vllm.utils import random_uuid
    VLLM_AVAILABLE = True
except ImportError:
    logging.error("vLLM n'est pas disponible - installation requise: pip install vllm")
    VLLM_AVAILABLE = False

from transformers import AutoTokenizer
from PIL import Image
import base64
import io

from ..schemas.openai_schemas import ChatMessage, ChatCompletionRequest

logger = logging.getLogger(__name__)


class QwenVLLMManager:
    """Gestionnaire du mod√®le Qwen3-VL avec vLLM"""
    
    def __init__(self):
        self.llm = None
        self.tokenizer = None
        self.model_name = os.getenv("MODEL_NAME", "Qwen/Qwen2-VL-2B-Instruct")
        self.max_model_len = int(os.getenv("MAX_MODEL_LEN", "8192"))
        self.gpu_memory_utilization = float(os.getenv("GPU_MEMORY_UTILIZATION", "0.85"))
        self.loading = False
        self.ready = False
        self.load_start_time = None
        
        # Semaphore pour limiter les inf√©rences concurrentes 
        self.inference_semaphore = asyncio.Semaphore(1)
        
        logger.info(f"QwenVLLMManager initialis√© avec {self.model_name}")
        
    async def load_model(self) -> None:
        """Charge le mod√®le vLLM de mani√®re asynchrone"""
        if self.loading or self.ready:
            return
            
        if not VLLM_AVAILABLE:
            raise RuntimeError("vLLM n'est pas disponible - installation requise")
        
        self.loading = True
        self.load_start_time = time.time()
        logger.info(f"üöÄ Chargement du mod√®le {self.model_name} avec vLLM")
        
        try:
            # Chargement dans un thread pool pour √©viter le blocage
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._load_model_sync)
            
            self.ready = True
            load_time = time.time() - self.load_start_time
            logger.info(f"‚úÖ Mod√®le vLLM charg√© avec succ√®s en {load_time:.2f}s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement vLLM: {e}")
            raise
        finally:
            self.loading = False
            
    def _load_model_sync(self) -> None:
        """Chargement synchrone du mod√®le vLLM"""
        try:
            # Configuration vLLM conservative pour √©viter les segfaults
            vllm_args = {
                "model": self.model_name,
                "max_model_len": min(self.max_model_len, 4096),  # Limite plus conservatrice
                "gpu_memory_utilization": min(self.gpu_memory_utilization, 0.7),  # M√©moire r√©duite
                "trust_remote_code": True,  # Requis pour Qwen2-VL
                "dtype": "half",  # float16 pour √©conomiser la m√©moire
                "enforce_eager": True,  # Mode eager pour √©viter les optimisations probl√©matiques
                "disable_custom_all_reduce": True,  # D√©sactiver les optimisations
                "swap_space": 4,  # Espace de swap pour la m√©moire
            }
            
            logger.info(f"Configuration vLLM: {vllm_args}")
            
            # Initialisation du mod√®le vLLM
            self.llm = LLM(**vllm_args)
            
            # Chargement du tokenizer pour les utilitaires
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            logger.info(f"‚úÖ vLLM initialis√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur dans _load_model_sync: {e}")
            raise
    
    async def wait_for_model(self, timeout: float = 300.0) -> bool:
        """Attend que le mod√®le soit pr√™t"""
        start_time = time.time()
        while not self.ready and (time.time() - start_time) < timeout:
            if not self.loading:
                asyncio.create_task(self.load_model())
            await asyncio.sleep(1)
        return self.ready
    
    def _prepare_messages_for_qwen(
        self, 
        messages: List[ChatMessage], 
        tools: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """Pr√©pare les messages au format Qwen3-VL"""
        conversation_parts = []
        
        # Message syst√®me avec outils si fournis
        system_content = "Tu es Qwen, un assistant IA utile cr√©√© par Alibaba Cloud."
        if tools:
            tools_description = "\n\nOutils disponibles:\n"
            for tool in tools:
                func = tool["function"]
                tools_description += f"- {func['name']}: {func.get('description', '')}\n"
            system_content += tools_description
            system_content += "\nPour utiliser un outil, r√©ponds avec un appel de fonction au format JSON."
        
        conversation_parts.append(f"<|im_start|>system\n{system_content}<|im_end|>")
        
        # Conversion des messages
        for msg in messages:
            role = msg.role
            content = ""
            
            if isinstance(msg.content, list):
                # Support multimodal : traitement des images et texte
                for part in msg.content:
                    if part.get("type") == "text":
                        content += part["text"]
                    elif part.get("type") == "image_url":
                        # Pour vLLM avec Qwen3-VL, nous incluons l'image
                        image_url = part.get("image_url", {}).get("url", "")
                        if image_url.startswith("data:"):
                            content += f"\n<image>{image_url}</image>\n"
                        else:
                            content += f"\n<image>{image_url}</image>\n"
            else:
                content = msg.content or ""
            
            conversation_parts.append(f"<|im_start|>{role}\n{content}<|im_end|>")
        
        # Ajout du token assistant pour la g√©n√©ration
        conversation_parts.append("<|im_start|>assistant\n")
        
        return "\n".join(conversation_parts)
    
    async def generate_response(
        self, 
        request: ChatCompletionRequest
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """G√©n√®re une r√©ponse avec vLLM de mani√®re asynchrone"""
        if not self.ready:
            await self.wait_for_model()
        
        async with self.inference_semaphore:
            try:
                # Pr√©paration du prompt
                prompt = self._prepare_messages_for_qwen(
                    request.messages, 
                    request.tools
                )
                
                # Configuration des param√®tres de g√©n√©ration
                sampling_params = SamplingParams(
                    temperature=request.temperature or 0.7,
                    max_tokens=request.max_tokens or 1024,
                    top_p=request.top_p or 0.8,
                    stop=["<|im_end|>", "<|endoftext|>"],
                    skip_special_tokens=False
                )
                
                # G√©n√©ration avec vLLM
                loop = asyncio.get_event_loop()
                outputs = await loop.run_in_executor(
                    None, 
                    self._generate_sync, 
                    prompt, 
                    sampling_params,
                    request.stream
                )
                
                if request.stream:
                    # Mode streaming : √©mulation pour l'instant
                    # vLLM supporte le streaming mais n√©cessite AsyncLLMEngine
                    generated_text = outputs[0].outputs[0].text
                    words = generated_text.split()
                    
                    for i, word in enumerate(words):
                        chunk = {
                            "delta": {"content": word + " " if i < len(words) - 1 else word},
                            "finish_reason": "stop" if i == len(words) - 1 else None
                        }
                        yield chunk
                        await asyncio.sleep(0.01)
                else:
                    # Mode non-streaming
                    generated_text = outputs[0].outputs[0].text.strip()
                    
                    # Analyse pour d√©tecter les function calls
                    function_calls = self._extract_function_calls(generated_text)
                    
                    yield {
                        "content": generated_text,
                        "function_calls": function_calls,
                        "finish_reason": "stop"
                    }
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur lors de la g√©n√©ration vLLM: {e}")
                yield {
                    "error": str(e),
                    "type": "generation_error"
                }
    
    def _generate_sync(
        self, 
        prompt: str, 
        sampling_params: SamplingParams,
        stream: bool = False
    ):
        """G√©n√©ration synchrone avec vLLM"""
        try:
            # G√©n√©ration avec vLLM
            outputs = self.llm.generate([prompt], sampling_params)
            return outputs
            
        except Exception as e:
            logger.error(f"‚ùå Erreur dans _generate_sync: {e}")
            raise
    
    def _extract_function_calls(self, text: str) -> List[Dict[str, Any]]:
        """Extrait les appels de fonction du texte g√©n√©r√©"""
        function_calls = []
        
        # Pattern pour d√©tecter les JSON de function calls
        import re
        json_pattern = r'\{[^}]*"name"\s*:\s*"[^"]+"[^}]*\}'
        matches = re.findall(json_pattern, text, re.DOTALL)
        
        for match in matches:
            try:
                func_call = json.loads(match)
                if "name" in func_call:
                    function_calls.append(func_call)
            except json.JSONDecodeError:
                continue
        
        return function_calls
    
    def get_model_info(self) -> Dict[str, Any]:
        """Retourne les informations du mod√®le"""
        return {
            "model_name": self.model_name,
            "ready": self.ready,
            "loading": self.loading,
            "backend": "vLLM",
            "max_model_len": self.max_model_len,
            "gpu_memory_utilization": self.gpu_memory_utilization,
            "multimodal": True,
            "supports_function_calling": True,
            "load_time": time.time() - self.load_start_time if self.load_start_time else None
        }
    
    async def cleanup(self):
        """Nettoyage des ressources"""
        if self.llm is not None:
            del self.llm
            self.llm = None
        
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        logger.info("Ressources vLLM nettoy√©es")


# Instance globale du gestionnaire de mod√®le
model_manager = QwenVLLMManager()