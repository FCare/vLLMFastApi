# Docker Compose pour llama.cpp avec Qwen3-VL
services:
  thebrain:
    build:
      context: .
      dockerfile: Dockerfile
    image: llama-qwen:latest
    container_name: thebrain
    
    # Configuration NVIDIA Runtime - OBLIGATOIRE pour GPU
    runtime: nvidia
    
    # Variables d'environnement GPU
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - CUDA_VISIBLE_DEVICES=0
    
    # Port exposé
    ports:
      - "${HOST_PORT:-8000}:8000"
    
    # Configuration mémoire pour GGUF Q4_K_XL (~5GB)
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
    
    # Santé et redémarrage
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 2m  # Temps pour le téléchargement du modèle
    
    restart: unless-stopped
    
    # Logs
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
